\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={7in, 8in}]{geometry}

\newcommand{\intt}{\ensuremath{\int\limits_0^\beta}\ d\tau}
\DeclareMathOperator{\atan}{atan}
\DeclareMathOperator{\sign}{sign}

% Title Page
\title{Stochastic Optimization Method for Analytic Continuation: implementation notes}
\author{Igor Krivenko}

\begin{document}
\maketitle

This document is a diverse collection of notes, explaining various details of the present SOM implementation.
It is supposed to serve as an addition to the lecture \cite{som_lecture}.

\section{Supported integral kernels}
\label{kernels}
\subsection{Fermionic thermal Green's function in imaginary time}
\label{fermiongf_imtime}

\begin{equation}
	G(\tau) = -\int\limits_{-\infty}^\infty
	d\epsilon \frac{e^{-\tau\epsilon}}{1+e^{-\beta\epsilon}} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(\tau;\omega)*R(c,w,h) =
	h(\Lambda(\tau;c+w/2) - \Lambda(\tau;c-w/2)),
\end{equation}
\begin{equation}
	\Lambda(\tau;\Omega) = -\frac{1}{\beta}
	\int\limits_0^{\beta\Omega}\frac{e^{-(\tau/\beta)x}}{1+e^{-x}}dx.
\end{equation}
The integral on the right hand side is analytically doable only for
a few values of $\alpha=\tau/\beta$, namely $\alpha=0,1/2,1$.
For all other $\alpha\in(0;1)$, it has to be done numerically and approximated using a cubic spline interpolation. The spline interpolation
would be easy to construct if the integrand were localized near $x=0$ for
all $\alpha$. However, this is not the case. The integrand develops
a long ``tail'' on the positive/negative half-axis as $\alpha$ approaches
0/1 respectively. The length of this tail scales as $\alpha^{-1}$ (or $(1-\alpha)^{-1}$), which would require an excessively large number of
spline knots in order to construct a reliable approximation.

The issue is solved by subtracting a tail contribution $t_\alpha(x)$ from the integrand, such that the difference is well localized, and an integral of $t_\alpha(x)$ is trivial.
\begin{equation}
	\Lambda(\tau;\Omega) = -\frac{1}{\beta} \left[
	\theta(-\Omega)S^-_\alpha(\beta\Omega) +
	\theta(\Omega)S^+_\alpha(\beta\Omega) +
	T_\alpha(\beta\Omega)
	\right],
\end{equation}
\begin{equation}
	S^-_\alpha(z) = -\int\limits_z^0
	\left[\frac{e^{-\alpha x}}{1+e^{-x}} - t_\alpha(x)
	\right] dx,\quad
	S^+_\alpha(z) = \int\limits_0^z
	\left[\frac{e^{-\alpha x}}{1+e^{-x}} - t_\alpha(x)
	\right] dx,
\end{equation}
\begin{equation}
	T_\alpha(z) = \int\limits_0^z t_\alpha(x) dx.
\end{equation}

The integrals $S^-_\alpha(z)$ and $S^+_\alpha(z)$ are precomputed on
a mesh with a fixed number of points, and provide input data for
spline construction. Limits of the meshes are $\alpha$-independent,
\begin{equation}
	S^-_\alpha(z): z\in[-x_0,0], \quad
	S^+_\alpha(z): z\in[0;x_0], \quad
	x_0 = -2\log(\mathtt{tolerance}).
\end{equation}
$x_0$ is chosen for the least localized case of $\alpha=1/2$.
We need two different splines (functions $S^\pm_\alpha(z)$) for positive and negative $z$ because the integrand is discontinuous (except for $\alpha=1/2$).

The tail contributions $t_\alpha(x)$ and all related quantities we use are
summarized in a table below.

\newcommand{\auxsum}[1]{\ensuremath{\sum_{n=0}^\infty(-1)^n\frac{e^{#1z}}{#1}}}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\alpha$ & $t_\alpha(x)$ & $T_\alpha(z)$ & $S_\alpha^-(z)$ & $S_\alpha^+(z)$\\
\hline
$0$ & $\theta(x)$ & $\theta(z)z$ &
$\log(1+e^{z})-\log(2)$ & $\log(1+e^{-z}) - \log(2)$ \\
\hline
$(0;1/2)$ & $\theta(x)e^{-\alpha x}$ & $\theta(z)\frac{e^{-\alpha z}-1}{-\alpha}$ &
$\auxsum{(n+1-\alpha)}-\Psi(1-\alpha)$ & $-\auxsum{-(n+1+\alpha)}-\Psi(1+\alpha)$ \\
\hline
$1/2$ & $0$ & $0$ & $2\atan(e^{z/2})-\pi/2$ & $-2\atan(e^{-z/2})+\pi/2$ \\
\hline
$(1/2;1)$ & $\theta(-x)e^{(1-\alpha)x}$ & $\theta(-z)\frac{e^{(1-\alpha)z}-1}{1-\alpha}$ & $-\auxsum{(n+2-\alpha)}+\Psi(2-\alpha)$ & $\auxsum{-(n+\alpha)}+\Psi(\alpha)$ \\
\hline
$1$ & $\theta(-x)$ & $\theta(-z)z$ &
$-\log(1+e^z) + \log(2)$ & $-\log(1+e^{-z}) + \log(2)$ \\
\hline
\end{tabular}
\end{center}

All series found in the table are rapidly convergent, $\Psi(z) = \frac{1}{2}[\psi(\frac{1+z}{2}) - \psi(\frac{z}{2})]$, $\psi(z)$ is the digamma function.

\subsection{Fermionic thermal Green's function at imaginary frequencies}
\label{fermiongf_imfreq}
\begin{equation}
	G(i\omega_n) = \int\limits_{-\infty}^\infty
	d\epsilon \frac{1}{i\omega_n-\epsilon} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(i\omega_n;\omega)*R(c,w,h) =
	h \log\left(\frac{i\omega_n - c + w/2}{i\omega_n - c - w/2}\right).
\end{equation}

\subsection{Fermionic thermal Green's function in Legendre polynomial basis}
\label{fermiongf_legendre}
\begin{equation}
	G(\ell) = -\int\limits_{-\infty}^\infty
	d\epsilon \frac{\beta\sqrt{2\ell+1}(-\mathrm{sgn}(\epsilon))^\ell i_{\ell}(\beta|\epsilon|/2)}
	{2\cosh(\beta\epsilon/2)} A(\epsilon)
\end{equation}
$i_\ell(x)$ is the modified spherical Bessel function of the first kind.

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(\ell;\omega)*R(c,w,h) = h (\Lambda(\ell;c+w/2) - \Lambda(\ell;c-w/2))
\end{equation}
\begin{equation}
	\Lambda(\ell;\Omega) = (-\mathrm{sgn}(\Omega))^{\ell+1}\sqrt{2\ell+1}
	\int_0^{|\Omega|\beta/2} \frac{i_\ell(x)}{\cosh(x)} dx
\end{equation}

The integral $\int_0^z \frac{i_\ell(x)}{\cosh(x)} dx$ is rather inconvenient for fast numerical evaluation. It cannot be expressed in terms of elementary functions or of a quickly converging series. Moreover, it grows logarithmically for $z\to\infty$, which makes spline interpolation infeasible.

These difficulties urge us to use a combined evaluation scheme. The scheme is based on the following expression for the integrand:
\begin{equation}\label{il_cosh_series}
	\frac{i_\ell(x)}{\cosh(x)} =
	\frac{e^x}{e^x+e^{-x}}\sum_{k=0}^\ell(-1)^k
		\frac{a_k(\ell+1/2)}{x^{k+1}} +
	\frac{e^{-x}}{e^x+e^{-x}}\sum_{k=0}^\ell(-1)^{\ell+1}
		\frac{a_k(\ell+1/2)}{x^{k+1}},
\end{equation}
where $a_k(\ell+1/2)$ are coefficients of the Bessel polynomials,
\begin{equation}
	a_k(\ell+1/2) = \frac{(l+k)!}{2^k k!(l-k)!} =
	\prod_{i=1}^k \frac{1}{2i}[\ell-k+2i-1][\ell-k+2i].
\end{equation}

For large $x$ (high-frequency region) the integrand can be approximated as
\begin{equation}\label{il_cosh_series_high}
	\frac{i_\ell(x)}{\cosh(x)} \approx
		\sum_{k=0}^\ell(-1)^k \frac{a_k(\ell+1/2)}{x^{k+1}}.
\end{equation}

For each $\ell$ the boundary value $x_0$ between the low- and high- frequency regions is found by a direct evaluation of (\ref{il_cosh_series}) and (\ref{il_cosh_series_high}). $x_0\approx15$ for $\ell=0$ (discrepancy on the level of $10^{-13}$) and grows as $(\mathrm{const}+\ell)^2$ for higher $\ell$.

In the low-energy region we do the integral $F^<(z) \equiv \int_0^z \frac{i_\ell(x)}{\cosh(x)} dx$ on a fixed $z$-mesh, $z\in[0;x_0]$, using the adaptive Simpson's method. Results of the integrations are used to construct a cubic spline interpolation of $F^<(z)$.

In the high-energy region we use the asymptotic form (\ref{il_cosh_series_high}) to do the integral,
\begin{equation}
	F^>(z)|_{z>x_0} = F^<(x_0) +
		\int_{x_0}^z dx \sum_{k=0}^\ell(-1)^k \frac{a_k(\ell+1/2)}{x^{k+1}},
\end{equation}
\begin{equation}
	\int_{x_0}^z dx \sum_{k=0}^\ell(-1)^k \frac{a_k(\ell+1/2)}{x^{k+1}} =
	\left.\left\{
		\log(x) +
		\sum_{k=1}^\ell (-1)^{k+1}\frac{a_k(\ell+1/2)}{x^k k}
	\right\}\right|_{x_0}^z.
\end{equation}

\subsection{Correlator of boson-like operators in imaginary time}
\label{bosoncorr_imtime}
\begin{equation}
	\chi(\tau) = \int\limits_{-\infty}^\infty \frac{d\epsilon}{\pi}
	\frac{\epsilon e^{-\tau\epsilon}}{1-e^{-\beta\epsilon}} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
\hat K(\tau;\omega)*R(c,w,h) =
h(\Lambda(\tau;c+w/2) - \Lambda(\tau;c-w/2)),
\end{equation}
\begin{equation}
\Lambda(\tau;\Omega) = \frac{1}{\pi\beta^2}
\int\limits_0^{\beta\Omega}\frac{xe^{-(\tau/\beta)x}}{1-e^{-x}}dx.
\end{equation}

Here we employ a tail subtraction technique analogous to that of paragraph
\ref{fermiongf_imtime}. All notations remain similar.

\begin{equation}
	\Lambda(\tau;\Omega) = \frac{1}{\pi\beta^2} \left[
	\theta(-\Omega)S^-_\alpha(\beta\Omega) +
	\theta(\Omega)S^+_\alpha(\beta\Omega) +
	T_\alpha(\beta\Omega)
	\right],
\end{equation}
\begin{equation}
	S^-_\alpha(z) = -\int\limits_z^0
	\left[\frac{xe^{-\alpha x}}{1-e^{-x}} - t_\alpha(x)
	\right] dx,\quad
	S^+_\alpha(z) = \int\limits_0^z
	\left[\frac{xe^{-\alpha x}}{1-e^{-x}} - t_\alpha(x)
	\right] dx,
\end{equation}
\begin{equation}
	T_\alpha(z) = \int\limits_0^z t_\alpha(x) dx.
\end{equation}

\newcommand{\dilog}{\ensuremath{\mathrm{Li}_2}}
\begin{center}
	\footnotesize
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		$\alpha$ & $t_\alpha(x)$ & $T_\alpha(z)$ & $S_\alpha^-(z)$ & $S_\alpha^+(z)$\\
		\hline
		$0$ & $\theta(x)x$ & $\theta(z)z^2/2$ &
		$-\pi^2/6+z\log(1-e^z)+\dilog(e^z)$ & $\pi^2/6+z\log(1-e^{-z})-\dilog(e^{-z})$ \\
		\hline
		$(0;1/2)$ & $\theta(x)xe^{-\alpha x}$ &
		$\theta(z)\frac{1-e^{-\alpha z}(1+z\alpha)}{\alpha^2}$ &
		$\begin{aligned}\sum_{n=0}^\infty\frac{e^{(n+1-\alpha)z}[1-z(n+1-\alpha)]}{(n+1-\alpha)^2}
		-\\-\psi^{(1)}(1-\alpha)\end{aligned}$ & $\begin{aligned}-\sum_{n=0}^\infty\frac{e^{-(n+1+\alpha)z}[1+z(n+1+\alpha)]}{(n+1+\alpha)^2}
		+\\+\psi^{(1)}(1+\alpha)\end{aligned}$ \\
		\hline
		$1/2$ & $0$ & $0$ & $\begin{aligned}-\pi^2/2+z\log(\tanh(-z/4))-\\-\dilog(e^z)+4\dilog(e^{z/2})\end{aligned}$ &
		$\begin{aligned}\pi^2/2+z\log(\tanh(z/4))+\\+\dilog(e^{-z})-4\dilog(e^{-z/2})\end{aligned}$ \\
		\hline
		$(1/2;1)$ & $\theta(-x)(-x)e^{(1-\alpha)x}$ &
		$\theta(-z)\frac{e^{(1-\alpha)z}(1-z(1-\alpha))-1}{(1-\alpha)^2}$ &
		$\begin{aligned}\sum_{n=0}^\infty\frac{e^{(n+2-\alpha)z}[1-z(n+2-\alpha)]}{(n+2-\alpha)^2}
		-\\-\psi^{(1)}(2-\alpha)\end{aligned}$ & $\begin{aligned}-\sum_{n=0}^\infty\frac{e^{-(n+\alpha)z}[1+z(n+\alpha)]}{(n+\alpha)^2}
		+\\+\psi^{(1)}(\alpha)\end{aligned}$ \\
		\hline
		$1$ & $\theta(-x)(-x)$ & $\theta(-z)(-z^2/2)$ &
		$-\pi^2/6+z\log(1-e^z)+\dilog(e^z)$ & $\pi^2/6+z\log(1-e^{-z})-\dilog(e^{-z})$ \\
		\hline
	\end{tabular}
\end{center}

All series found in the table are rapidly convergent, $\dilog(z)$ is the dilogarithm (Spence's function),
$\psi^{(1)}(z) = \frac{d^2}{dz^2}\log\Gamma(z)$ is the trigamma function.

Factor $x$ in the kernel tend to strengthen its delocalization, so
it makes sense to choose $x_0$ (boundary of the spline interpolation region)
a bit larger, for instance, $x_0=-2.3\log(\mathtt{tol})$.

\subsection{Correlator of boson-like operators at imaginary frequencies}
\label{bosoncorr_imfreq}
\begin{equation}
\chi(i\Omega_n) = \int\limits_{-\infty}^\infty
d\epsilon \frac{1}{\pi}\frac{-\epsilon}{i\Omega_n-\epsilon} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
\hat K(i\Omega_n;\omega)*R(c,w,h) =
\frac{hw}{\pi} + \frac{hi\Omega_n}{\pi}\log\left(\frac{i\Omega_n - c - w/2}{i\Omega_n - c + w/2}\right).
\end{equation}

\subsection{Correlator of boson-like operators in Legendre polynomial basis}
\label{bosoncorr_legendre}
\begin{equation}
	\chi(\ell) = \int\limits_{-\infty}^\infty
	\frac{d\epsilon}{\pi}
	\frac{\beta\epsilon\sqrt{2\ell+1}(-\mathrm{sgn}(\epsilon))^\ell i_{\ell}(\beta|\epsilon|/2)}
	{2\sinh(\beta\epsilon/2)} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(\ell;\omega)*R(c,w,h) = h (\Lambda(\ell;c+w/2) - \Lambda(\ell;c-w/2))
\end{equation}
\begin{equation}
	\Lambda(\ell;\Omega) = -(-\mathrm{sgn}(\Omega))^{\ell+1}
	\frac{2\sqrt{2\ell+1}}{\pi\beta}
	\int_0^{|\Omega|\beta/2} \frac{i_\ell(x) x}{\sinh(x)} dx
\end{equation}

Once again, we apply the combined evaluation scheme described in
\ref{fermiongf_legendre}. An explicit expression for the integrand reads
\begin{equation}\label{il_x_sinh_series}
	\frac{i_\ell(x) x}{\sinh(x)} =
	\frac{e^x}{e^x-e^{-x}}\sum_{k=0}^\ell(-1)^k
	\frac{a_k(\ell+1/2)}{x^k} +
	\frac{e^{-x}}{e^x-e^{-x}}\sum_{k=0}^\ell(-1)^{\ell+1}
	\frac{a_k(\ell+1/2)}{x^k}.
\end{equation}
Its high-frequency asymptotics is
\begin{equation}
	\frac{i_\ell(x) x}{\sinh(x)} \approx
		\sum_{k=0}^\ell(-1)^k \frac{a_k(\ell+1/2)}{x^k}.
\end{equation}

Integral $F^<(z) \equiv \int_0^z \frac{i_\ell(x)x}{\sinh(x)} dx$ is done numerically,
and replaced by a cubic spline. For $z>x_0$ we have to add a tail contribution,
\begin{equation}
F^>(z)|_{z>x_0} = F^<(x_0) +
\left.\left\{
x - \frac{\ell(\ell+1)}{2}\log(x) +
\sum_{k=1}^{\ell-1} (-1)^k\frac{a_{k+1}(\ell+1/2)}{x^k k}
\right\}\right|_{x_0}^z.
\end{equation}

\subsection{Autocorrelator of a boson-like operator in imaginary time}
\label{bosonautocorr_imtime}
\begin{equation}
\chi(\tau) = \int\limits_{-\infty}^\infty \frac{d\epsilon}{\pi}
\frac{\epsilon (e^{-\tau\epsilon}+e^{-(\beta-\tau)\epsilon})}
{1-e^{-\beta\epsilon}} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
\hat K(\tau;\omega)*R(c,w,h) =
h(\Lambda(\tau;c+w/2) - \Lambda(\tau;c-w/2)),
\end{equation}
\begin{equation}
\Lambda(\tau;\Omega) = \frac{1}{\pi\beta^2}
\int\limits_0^{\beta\Omega}\frac{x(e^{-(\tau/\beta)x}+e^{-(1-\tau/\beta)x})}
{1-e^{-x}}dx.
\end{equation}
This integral is obtained from the one of paragraph \ref{bosoncorr_imtime}
as a result of $\tau$-symmetrization (sum of $\Lambda$ for $\alpha$ and $1-\alpha$).
Thanks to this fact, it is enough to consider only 3 $\alpha$-cases,
\begin{itemize}
	\item $\alpha=0,1$;
	\item $\alpha=1/2$;
	\item $\alpha\in(0;1/2)\cup(1/2;1)$.
\end{itemize}
Furthermore, the upper integration limit can now be only positive, which
eliminates the need in two cubic splines.

\begin{equation}
\Lambda(\tau;\Omega) = \frac{1}{\pi\beta^2} \left[
S_\alpha(\beta\Omega) + T_\alpha(\beta\Omega) \right],
\end{equation}
\begin{equation}
S_\alpha(z) = \int\limits_0^z
\left[\frac{x(e^{-\alpha x}+e^{-(1-\alpha)x})}{1-e^{-x}} - t_\alpha(x)
\right] dx, \quad
T_\alpha(z) = \int\limits_0^z t_\alpha(x) dx.
\end{equation}

\begin{center}
	\footnotesize
	\begin{tabular}{|c|c|c|c|}
		\hline
		$\alpha$ & $t_\alpha(x)$ & $T_\alpha(z)$ & $S_\alpha(z)$ \\
		\hline
		$0,1$ & $x(1+e^{-x})$ & $1+z^2/2-e^{-z}(1+z)$ &
		$-1+\pi^2/3+2z\log(1-e^{-z})-2\dilog(e^{-z})+e^{-z}(1+z)$ \\
		\hline
		$(0;1/2)\cup(1/2;1)$ & $x(e^{-\alpha x}+e^{-(1-\alpha)x})$ &
		$\begin{array}{c}\frac{1-e^{-\alpha z}(1+z\alpha)}{\alpha^2}+\\+
		\frac{1-e^{-(1-\alpha) z}(1+z(1-\alpha))}{(1-\alpha)^2}\end{array}$ &$\begin{aligned}-\sum_{n=0}^\infty\frac{e^{-(n+1+\alpha)z}[1+z(n+1+\alpha)]}
		{(n+1+\alpha)^2} +\psi^{(1)}(1+\alpha) -\\
		-\sum_{n=0}^\infty\frac{e^{-(n+2-\alpha)z}[1+z(n+2-\alpha)]}
		{(n+2-\alpha)^2} +\psi^{(1)}(2-\alpha)\end{aligned}$ \\
		\hline
		$1/2$ & $2xe^{-x/2}$ & $4(2-e^{-z/2}(2+z))$ &
		$\begin{aligned}-8+\pi^2+4e^{-z/2}(2+z)+2z\log\tanh(z/4)-\\
		-8\dilog(e^{-z/2})+2\dilog(e^{-z})\end{aligned}$ \\
		\hline
	\end{tabular}
\end{center}

All series found in the table are rapidly convergent, $\dilog(z)$ is the dilogarithm (Spence's function),
$\psi^{(1)}(z) = \frac{d^2}{dz^2}\log\Gamma(z)$ is the trigamma function.

\subsection{Autocorrelator of a boson-like operator at imaginary frequencies}
\label{bosonautocorr_imfreq}
\begin{equation}
\chi(i\Omega_n) = \int\limits_0^\infty
d\epsilon \frac{1}{\pi}\frac{2\epsilon^2}{\Omega_n^2+\epsilon^2} A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
\hat K(i\Omega_n;\omega)*R(c,w,h) =
\frac{2hw}{\pi} + \frac{2h\Omega_n}{\pi}\left(
\atan\left(\frac{c-w/2}{\Omega_n}\right) - \atan\left(\frac{c+w/2}{\Omega_n}\right)
\right).
\end{equation}

\subsection{Autocorrelator of a boson-like operator in Legendre polynomial basis}
\label{bosonautocorr_legendre}
\begin{equation}
	\chi(\ell) = \left\{
		\begin{array}{ll}
		\int\limits_0^\infty
		\frac{d\epsilon}{\pi}
		\frac{\beta\epsilon\sqrt{2\ell+1} i_{\ell}(\beta\epsilon/2)}
		{\sinh(\beta\epsilon/2)} A(\epsilon), &\ell\ \mathrm{ even},\\
		0, &\ell\ \mathrm{odd}.
	\end{array}\right.
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(\ell;\omega)*R(c,w,h) = \left\{
	\begin{array}{ll}
	h(\Lambda(\ell;c+w/2)-\Lambda(\ell;c-w/2)), &\ell\ \mathrm{ even},\\
	0, &\ell\ \mathrm{odd}.
	\end{array}\right.
\end{equation}
\begin{equation}
	\Lambda(\ell;\Omega) =
	\frac{4\sqrt{2\ell+1}}{\pi\beta}
	\int_0^{\Omega\beta/2} \frac{i_\ell(x) x}{\sinh(x)} dx.
\end{equation}

See paragraph \ref{bosoncorr_legendre} for a description of how the integral on
the right hand side is evaluated.

\subsection{Zero temperature correlator in imaginary time}
\label{zerotemp_imtime}
\begin{equation}
	G(\tau) = -\int\limits_0^\infty
	d\epsilon\ e^{-\tau\epsilon} A(\epsilon), \quad
	\tau\in[0;\tau_{max}]
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(\tau;\omega)*R(c,w,h) =  \left\{
	\begin{array}{ll}
		-hw, &\tau=0,\\
		\frac{h}{\tau}(e^{-\tau(c+w/2)}-e^{-\tau(c-w/2)}) ,&\mathrm{otherwise}.
	\end{array}\right.
\end{equation}

\subsection{Zero temperature correlator at imaginary frequencies}
\label{zerotemp_imfreq}
\begin{equation}
	G(i\omega_n) = \int\limits_0^\infty
	d\epsilon \frac{1}{i\omega_n-\epsilon} A(\epsilon).
\end{equation}

Imaginary frequency values $\omega_n$ are arbitrary here, with a restriction that
$\omega_0 = 0$ is not included in the case of fermionic correlators.

Integral kernel applied to a rectangle,
\begin{equation}
\hat K(i\omega_n;\omega)*R(c,w,h) =
h \log\left(\frac{i\omega_n - c + w/2}{i\omega_n - c - w/2}\right).
\end{equation}

\subsection{Zero temperature correlator in Legendre polynomial basis}
\label{zerotemp_legendre}
\begin{equation}
	G(\ell) =
	\int\limits_0^\infty d\epsilon
	\tau_{max}(-1)^{\ell+1}\sqrt{2\ell+1}
	i_{\ell}\left(\frac{\epsilon\tau_{max}}{2}\right)
	\exp\left(-\frac{\epsilon\tau_{max}}{2}\right) A(\epsilon).
\end{equation}

Integral kernel applied to a rectangle,
\begin{equation}
	\hat K(\ell;\omega)*R(c,w,h) = h (\Lambda(\ell;c+w/2) - \Lambda(\ell;c-w/2)),
\end{equation}
\begin{equation}
	\Lambda(\ell;\Omega) = (-1)^{\ell+1}\sqrt{2\ell+1}
	\int_0^{\Omega\tau_{max}/2} 2e^{-x} i_\ell(x) dx.
\end{equation}

Once again, we apply the combined evaluation scheme described in
\ref{fermiongf_legendre}. An explicit expression for the integrand reads
\begin{equation}\label{il_x_exp_series}
	2e^{-x} i_\ell(x) =
	\sum_{k=0}^\ell(-1)^k
	\frac{a_k(\ell+1/2)}{x^k} +
	e^{-2x}\sum_{k=0}^\ell(-1)^{\ell+1}
	\frac{a_k(\ell+1/2)}{x^k}.
\end{equation}
Its high-frequency asymptotics is
\begin{equation}
	2e^{-x} i_\ell(x) \approx
	\sum_{k=0}^\ell(-1)^k \frac{a_k(\ell+1/2)}{x^k}.
\end{equation}

Integral $F^<(z) \equiv \int_0^z 2 e^{-x} i_\ell(x) dx$ is done numerically,
and replaced by a cubic spline. For $z>x_0$ we have to add a tail contribution,
\begin{equation}
F^>(z)|_{z>x_0} = F^<(x_0) + \left.\left\{
\log(x) + \sum_{k=1}^\ell (-1)^{k+1}\frac{a_k(\ell+1/2)}{x^k k}
\right\}\right|_{x_0}^z.
\end{equation}

\section{Fit quality criterion for complex functions}
\label{fit_quality}

In section 4.1 Mishchenko introduces a special quantity $\kappa$, which characterizes the fit quality of a given particular solution
$A(\omega)$ (eq. 55).
\begin{equation}
	\kappa = \frac{1}{M-1}\sum_{m=2}^M\theta(-\Delta(m)\Delta(m-1)),
\end{equation}
where $\Delta(m) = (\tilde G(m) - G(m))/\mathcal{S}(m)$ is the deviation function. The proposed expression for $\kappa$ makes  apparently no sense for complex $G(m)$ and $\Delta(m)$, e.g. when $G$ is given as a function of Matsubara frequencies.

Our generalization consists in a replacement
\begin{multline}
	\theta(-\Delta(m)\Delta(m-1)) \mapsto
	\frac{1}{2}\left[1 - \frac{\Re[\Delta(m)\Delta^*(m-1)]}{|\Delta(m)\Delta^*(m-1)|} \right]=\\=
	\frac{1}{2}\left[ 1 - \cos[\arg(\Delta(m)) - \arg(\Delta(m-1))] \right].
\end{multline}
According to the modified definition, two adjacent values of $\Delta$ are considered anti-correlated, if the complex phase shift between them exceeds $\pm\pi/2$. In the case of real-valued quantities the phase shift is always either 0, or $\pi$.

\section{Probability density function for the parameter change}
\label{prob_function}

Section 3.4 of \cite{som_lecture} contains a dubious expression for the probability
density function $\mathcal{P}\sim (\max(|\delta\xi_\mathrm{min}|, |\delta\xi_\mathrm{max}|)/|\delta\xi|)^\gamma$, where $\gamma\gg1$. It is
obviously divergent at $\delta\xi\to0$ and cannot be properly normalized, if
ends of $[\delta\xi_\mathrm{min};\delta\xi_\mathrm{max}]$ segment have different signs.

We use a different density function, which is finite everywhere on $\delta\xi\in[\delta\xi_\mathrm{min};\delta\xi_\mathrm{max}]$, and is qualitatively similar to that of Mishchenko.
\begin{equation}
	\mathcal{P}(\delta\xi) = \mathcal{N}
	\exp\left(-\gamma \frac{|\delta\xi|}{L}\right), \quad
	L \equiv \max(|\delta\xi_\mathrm{min}|, |\delta\xi_\mathrm{max}|),
\end{equation}
\begin{equation}
	\mathcal{N} = \frac{\gamma}{L}\left[
		\sign(\delta\xi_\mathrm{min})(e^{-\gamma|\delta\xi_\mathrm{min}|/L} - 1) +
		\sign(\delta\xi_\mathrm{max})(1 - e^{-\gamma|\delta\xi_\mathrm{max}|/L})
	\right]^{-1}.
\end{equation}

\section{Hilbert(-like) transforms of a rectangular spectral function}
\label{hilbert_transform}

Recovery of observables as functions of the real frequency from a computed spectral function amounts to evaluating Hilbert(-like) transforms of rectangles $R(c,w,h)$. The transforms in question are
\begin{itemize}
    \item
    \begin{equation}
        F_1(\epsilon) =
        - \int_{-\infty}^\infty d\epsilon'
            \frac{R(c,w,h;\epsilon')}{\epsilon' - \epsilon - i0} =
        -h \int_{c-w/2}^{c+w/2} d\epsilon' \frac{1}{\epsilon' - \epsilon - i0} =
        -h \log\left(\frac{c + w / 2 - \epsilon}{c - w / 2 - \epsilon}\right)
    \end{equation}
    for fermionic thermal Green's functions and zero temperature correlators;

    \item
    \begin{equation}
        F_2(\epsilon) =
        - \int_{-\infty}^\infty d\epsilon'
        \frac{\epsilon' R(c,w,h;\epsilon')}{\epsilon' - \epsilon - i0} =
        -h \int_{c-w/2}^{c+w/2} d\epsilon'
            \frac{\epsilon'}{\epsilon' - \epsilon - i0} =
        -h \left[ w + \epsilon \log\left(\frac{c + w / 2 - \epsilon}{c - w / 2 - \epsilon}\right)\right]
    \end{equation}
    for (auto)correlators of boson-like operators.
\end{itemize}

When binning is used to perform projection of an observable onto a real frequency mesh, functions $F_1(\epsilon)$ and $F_2(\epsilon)$ are replaced by their averages over a finite segment $[\epsilon_a, \epsilon_b]$.
\begin{multline}
    \frac{1}{\epsilon_b - \epsilon_a} \int_{\epsilon_a}^{\epsilon_b}
        F_1(\epsilon) d\epsilon =\\=
        \frac{h}{\epsilon_b - \epsilon_a} \left[
            (\epsilon_- - \epsilon_a) \log(\epsilon_- - \epsilon_a) +
            (\epsilon_+ - \epsilon_b) \log(\epsilon_+ - \epsilon_b) -
            (\epsilon_+ - \epsilon_a) \log(\epsilon_+ - \epsilon_a) -
            (\epsilon_- - \epsilon_b) \log(\epsilon_- - \epsilon_b)
        \right],
\end{multline}
\begin{multline}
    \frac{1}{\epsilon_b - \epsilon_a} \int_{\epsilon_a}^{\epsilon_b}
    F_2(\epsilon) d\epsilon = -\frac{hw}{2} -\\-
    \frac{h}{2(\epsilon_b - \epsilon_a)} \left[
        (\epsilon_-^2 - \epsilon_b^2) \log(\epsilon_- - \epsilon_b) +
        (\epsilon_+^2 - \epsilon_a^2) \log(\epsilon_+ - \epsilon_a) -
        (\epsilon_+^2 - \epsilon_b^2) \log(\epsilon_+ - \epsilon_b) -
        (\epsilon_-^2 - \epsilon_a^2) \log(\epsilon_- - \epsilon_a)
    \right],
\end{multline}
where $\epsilon_\pm = c \pm w/2$ are edges of the rectangle.

\section{Consistent-constraints updates}
\label{cc_update}

Consistent-constraints (CC) updates introduced in SOM 2.0 are a new type of updates in the Markov chain used to accumulate particular solutions (Section II.A of \cite{socc_paper}). Unlike elementary updates, they radically change an MC configuration to help reveal local minima of the objective function more quickly.

The CC updates are proposed during stage A of a global update at regular intervals. Two consecutive CC updates are separated by \verb|cc_update_cycle_length| elementary updates. A CC update is skipped if it could potentially result in a configuration with too many rectangles, $2K_0+1>$ \verb|max_rects|, where $K_0$ is the number of rectangles in the current configuration.
CC updates are accepted and rejected based on the same Metropolis criterion as the elementary updates.
A proposed CC update proceeds as follows.
\begin{enumerate}
    \item Construct a non-overlapping configuration $\{(c_k, w_k, h_k)\}$ out of the current  configuration (Eqs. (16)-(17) of \cite{socc_paper}) and discard rectangles with width below \verb|min_rect_width|. If size $K$ of the resulting non-overlapping configuration is less than 3, reject the update.

    \item Collect heights $h_k$ of rectangles in the non-overlapping configuration in a $K$-dimensional vector $\mathbf{h}$.

    \item Starting from the collected heights, use the CC protocol (see below) to compute the optimal heights $\mathbf{h}^\mathrm{(opt)}$.

    \item If any of $h^\mathrm{(opt)}_k$ is significantly negative, i.e. $w_k h^\mathrm{(opt)}_k < -\verb|min_rect_weight|$, reject the update.

    \item Replace heights $h_k$ in the non-overlapping configuration with the optimal heights $h^\mathrm{(opt)}_k$.

    \item Remove small rectangles ($w_k h^\mathrm{(opt)}_k < \verb|min_rect_weight|$) from the configuration and redistribute their weight by changing heights of their neighboring rectangles. This step is repeated until all rectangles satisfy ($w_k h_k \geq \verb|min_rect_weight|$).

    \item Compute the $\chi^2$-functional (Eq.~(10) of \cite{socc_paper}) of the new optimized configuration, and use $Z(\chi^2 /\chi^2_\mathrm{(opt)})$ as the Metropolis ratio.
\end{enumerate}

The CC protocol consists in iterative minimization of a quadratic function of $h_k$ with self-consistent determination of regularization parameters $Q^{(0)}_k, Q^{(1)}_k, Q^{(2)}_k$ this function depends on. The function in question is a sum of the $\chi^2$-functional and a few regularization terms,
\begin{equation}
    O[\mathbf{h}; Q^{(0)},Q^{(1)},Q^{(2)}] = \chi^2[\mathbf{h}]
     + O_0[\mathbf{h};Q^{(0)}]
     + O_1[\mathbf{h};Q^{(1)}]
     + O_2[\mathbf{h};Q^{(2)}].
\end{equation}
The $\chi^2$-functional reads
\begin{equation}\label{cc_update:chi2}
    \chi^2[\mathbf{h}] = \frac{1}{N} \sum_{n=1}^{N}
        \frac{|\hat K A[\mathbf{h}] - g_n|^2}{\sigma^2_n},
\end{equation}
where $g_n$ is the discretized right-hand side of the analytic continuation problem, and $\sigma_n$ are respective estimated error bars on the input data. Action of the integral kernel $\hat K$ on a configuration $A[\mathbf{h}]$ is expressed in terms of the integrated kernel $\Lambda$,
\begin{equation}
    \hat K A[\mathbf{h}] = \sum_{k=1}^K h_k \Lambda_n(c_k, w_k), \quad
    \Lambda_n(c_k, w_k) \equiv
    \int\limits_{c_k-w_k/2}^{c_k+w_k/2} K(n,\epsilon) d\epsilon.
\end{equation}
Substituting this expression into the definition (\ref{cc_update:chi2}) and applying simple algebraic transformations, we get a quadratic function of $h_k$,
\begin{equation}
    \chi^2[\mathbf{h}] =
    \frac{1}{N} \sum_{n=1}^N \frac{1}{\sigma_n^2}
    \left(\sum_{k=1}^K h_k \Lambda_n^*(c_k, w_k) - g^*_n\right)
    \left(\sum_{k'=1}^K h_{k'} \Lambda_n(c_{k'}, w_{k'}) - g_n\right) =
    \mathbf{h}^T \hat O_{\chi^2} \mathbf{h}
    -2\mathbf{f}^T_{\chi^2} \mathbf{h} + \mathrm{const}.
\end{equation}
with
\begin{align}
    (\hat O_{\chi^2})_{kk'} &= \frac{1}{N} \sum_{n=1}^N \frac{1}{\sigma_n^2}
        \mathrm{Re}\left[
        \Lambda^*_n(c_k, w_k) \Lambda_n(c_{k'}, w_{k'})
        \right],\\
    (\mathbf{f}_{\chi^2})_k &= \frac{1}{N} \sum_{n=1}^N \frac{1}{\sigma_n^2}
        \mathrm{Re}\left[\Lambda^*_n(c_k, w_k) g_n\right].
\end{align}

The regularization term $O_0$ penalizes large amplitudes of a solution,
\begin{equation}
    O_0[\mathbf{h};Q^{(0)}] = \sum_{k=1}^K (Q^{(0)}_k)^2 h_k^2 =
        \mathbf{h}^T \hat O_{(0)} \mathbf{h}, \quad
        \hat O_{(0)} = \mathrm{diag}((Q^{(0)}_k)^2).
\end{equation}

$O_1$ and $O_2$ penalize large values of the 1st and the 2nd derivative of a solution respectively,
\begin{align}
    O_1[\mathbf{h};Q^{(1)}] &= \sum_{k=1}^{K-1}
        (Q^{(1)}_k)^2 |A'(\epsilon_k)|^2 =
        \mathbf{h}^T \hat O_{(1)} \mathbf{h},\\
    O_2[\mathbf{h};Q^{(2)}] &= \sum_{k=2}^{K-1}
        (Q^{(2)}_{k-1})^2 |A''(\epsilon_k)|^2 =
        \mathbf{h}^T \hat O_{(2)} \mathbf{h}.
\end{align}
Matrices $\hat O_{(1)}$ and $\hat O_{(2)}$ are derived from finite-difference approximations of $A'$ and $A''$ at points $\epsilon_k = c_k$. A forward-difference approximation
\begin{equation}\label{cc_update:finite_diff_forward}
    O_1[\mathbf{h};Q^{(1)}] \approx \sum_{k=1}^{K-1} (Q^{(1)}_k)^2
        \left(\frac{h_{k+1} - h_k}{c_{k+1} - c_k}\right)^2
\end{equation}
gives rise to matrix elements
\begin{align}
    (\hat O_{(1)})_{k,k} &=
    \left(\frac{Q^{(1)}_{k-1}}{c_k - c_{k-1}}\right)^2 \theta(k>1) +
    \left(\frac{Q^{(1)}_k}{c_{k+1} - c_{k}}\right)^2\theta(k<K),
    \quad k=\overline{1,K},\\
    (\hat O_{(1)})_{k,k+1} &= (\hat O_{(1)})_{k+1,k} =
    -\left(\frac{Q^{(1)}_k}{c_{k+1} - c_k}\right)^2, \quad k=\overline{1,K-1}.
\end{align}
Similarly, a second order symmetric difference approximation
\begin{equation}\label{cc_update:finite_diff_2_symm}
    O_2[\mathbf{h};Q^{(2)}] \approx \sum_{k=2}^{K-1} (Q^{(2)}_{k-1})^2
    \left(\frac
    {h_{k+1}(c_k - c_{k - 1}) + h_{k-1} (c_{k+1} - c_k) - h_k(c_{k+1} - c_{k-1})}
    {1/2(c_{k+1} - c_{k-1})(c_{k+1} - c_{k})(c_{k} - c_{k-1})}
    \right)^2
\end{equation}
results in matrix elements
\begin{align}
    (\hat O_{(2)})_{k,k} &=
    \frac{4 (Q^{(2)}_{k-2})^2}{((c_k - c_{k-2})(c_k - c_{k-1}))^2}\theta(k>2) +
    \frac{4 (Q^{(2)}_{k-1})^2}{((c_k - c_{k-1})(c_{k+1} - c_{k}))^2}\theta(k>1 \cap k<K) \nonumber +\\&+
    \frac{4 (Q^{(2)}_k)^2}{((c_{k+2} - c_k)(c_{k+1} - c_k))^2}\theta(k<K-1),
    \quad k=\overline{1,K},\\
    (\hat O_{(2)})_{k,k+1} &= (\hat O_{(2)})_{k+1,k} =
    \frac{-4(Q^{(2)}_{k-1})^2}{(c_{k+1} - c_{k-1})(c_k - c_{k-1})(c_{k+1} - c_k)^2}\theta(k>1) \nonumber +\\&+
    \frac{-4(Q^{(2)}_k)^2}{(c_{k+2} - c_k)(c_{k+1} - c_k)^2(c_{k+2} - c_{k+1})^2}\theta(k<K-1),
    \quad k=\overline{1,K-1},\\
    (\hat O_{(2)})_{k,k+2} &= (\hat O_{(2)})_{k+2,k} =
    \frac{4(Q^{(2)}_{k})^2}{(c_{k+2} - c_{k})^2(c_{k+1} - c_k)(c_{k+2} - c_{k+1})}, \quad k=\overline{1,K-2}.
\end{align}

CC protocol iterations are organized as follows.
\begin{enumerate}
    \item Precompute integrated kernel $\Lambda$, matrix $\hat O_{\chi^2}$ and vector $\mathbf{f}_{\chi^2}$.

    \item Initialize regularization parameters according to
    \begin{align*}
        Q^{(0)}_k &= 0, \quad k=\overline{1,K},\\
        Q^{(1)}_k &= (\mathtt{cc\_update\_der\_penalty\_init})W^2 / \mathcal{N}, \quad k = \overline{1,K-1},\\
        Q^{(2)}_k &= (\mathtt{cc\_update\_der\_penalty\_init})W^3 / \mathcal{N}, \quad k = \overline{1,K-2},
    \end{align*}
    where $W$ is the energy window width and $\mathcal{N}$ is the requested solution norm.

    \item The regularization parameters are used to compute matrix $\hat O = 
    \hat O_{\chi^2} + \hat O_{(0)} + \hat O_{(1)} + \hat O_{(2)}$.

    \item Minimize a quadratic function
    \begin{equation}
        \mathbf{h}^T \hat O \mathbf{h} -2\mathbf{f}^T_{\chi^2} \mathbf{h}
    \end{equation}
    w.r.t. $\mathbf{h}$ subject to the normalization sum rule $\sum_{k=1}^K w_k h_k = \mathcal{N}$. The minimization result is a new  vector $\mathbf{h}^\mathrm{new}$.

    \item Discrepancy of weights of rectangles with heights $\mathbf{h}$ and $\mathbf{h}^\mathrm{new}$ is computed as
    \begin{equation}
        \frac{1}{\mathcal{N}}\sum_{k=1}^K |w_k (h_k - h^\mathrm{new}_k)|.
    \end{equation}
    If it lies within a fixed tolerance level (\verb|cc_update_rect_norm_variation_tol|),
    then convergence is reached and iterations are terminated.

    \item Update amplitude regularization parameters $Q^{(0)}_k$: If $h^\mathrm{new}_k$ is negative, set\\
    $Q^{(0)}_k = (\mathtt{cc\_update\_height\_penalty\_max})W / \mathcal{N}$.
    Otherwise divide $Q^{(0)}_k$ by \verb|cc_update_height_penalty_divisor|.

    \item Use the same finite difference approximation as in Eq. (\ref{cc_update:finite_diff_forward}) to compute the 1st derivatives $d^{(1)}_k, k = \overline{1,K-1}$.

    \item Compute a limiter for $Q^{(1)}_k$,
    $Q^{(1)}_\mathrm{lim} = (\mathtt{cc\_update\_der\_penalty\_limiter}) \min_k Q^{(1)}_k$.

    \item Update 1st derivative regularization parameters $Q^{(1)}_k$:\\
    If $|d^{(1)}_k| > (\mathtt{cc\_update\_der\_penalty\_threshold} / \sqrt{K-1}) / Q^{(1)}_k$, then set\\
    $Q^{(1)}_k = (\mathtt{cc\_update\_der\_penalty\_threshold} / \sqrt{K-1}) / |d^{(1)}_k|$.\\
    Otherwise multiply $Q^{(1)}_k$ by \verb|cc_update_der_penalty_increase_coeff|.

    \item Reduce excessively large $Q^{(1)}_k$: If $Q^{(1)}_k > Q^{(1)}_\mathrm{lim}$, then set $Q^{(1)}_k = Q^{(1)}_\mathrm{lim}$.

    \item Use the same finite difference approximation as in Eq. (\ref{cc_update:finite_diff_2_symm}) to compute the 2nd derivatives $d^{(2)}_k, k = \overline{1,K-2}$.

    \item Compute a limiter for $Q^{(2)}_k$,
    $Q^{(2)}_\mathrm{lim} = (\mathtt{cc\_update\_der\_penalty\_limiter}) \min_k Q^{(2)}_k$.

    \item Update 2nd derivative regularization parameters $Q^{(2)}_k$:\\
    If $|d^{(2)}_k| > (\mathtt{cc\_update\_der\_penalty\_threshold} / \sqrt{K-1}) / Q^{(2)}_k$, then set\\
    $Q^{(2)}_k = (\mathtt{cc\_update\_der\_penalty\_threshold} / \sqrt{K-1}) / |d^{(2)}_k|$.\\
    Otherwise multiply $Q^{(2)}_k$ by \verb|cc_update_der_penalty_increase_coeff|.

    \item Reduce excessively large $Q^{(2)}_k$: If $Q^{(2)}_k > Q^{(2)}_\mathrm{lim}$, then set $Q^{(2)}_k = Q^{(2)}_\mathrm{lim}$.

    \item If the maximal allowed number of iterations (\verb|cc_update_max_iter|) is reached, terminate. Otherwise set $\mathbf{h} = \mathbf{h}^\mathrm{new}$ and repeat from step 3.
\end{enumerate}
The last vector $\mathbf{h}^\mathrm{new}$ obtained during iterations is taken to be the optimization result $\mathbf{h}^\mathrm{(opt)}$.

\section{Construction of final solution using the consistent-constraints protocol}
\label{final_solution_cc}

SOM accumulates particular solutions $A_j(\epsilon)$, selects those
fulfilling $\chi^2[A_j] \leq \chi^2_c$ ($\chi_c$ is set by user) and construct a final solution $A(\epsilon)$ as a linear combination
\begin{equation}
    A(\epsilon) = \sum_{j=1}^J c_j A_j(\epsilon), \quad
    \sum_{j=1}^J c_j = 1.
\end{equation}
Prior to SOM 2.0, the only available choice of the coefficients was $c_j = J^{-1}, \forall j=\overline{1,J}$. SOM 2.0 features an optimization procedure, which finds a vector of coefficients $\mathbf{c}$ resulting in a smoother final solution. It also allows to bias the final solution towards a user-provided default model.

The optimization procedure minimizes a quadratic function of $\mathbf{c}$ depending on a few sets of regularization parameters,
\begin{equation}
    O[\mathbf{c}; Q_k, D_k, B_k, T_k, A_T(\epsilon_k), U] =
        O_Q[\mathbf{c}; Q_k] +
        O_D[\mathbf{c}; D_k] +
        O_B[\mathbf{c}; B_k] +
        O_T[\mathbf{c}; T_k, A_T(\epsilon)] +
        O_U[\mathbf{c}; U].
\end{equation}
\begin{itemize}
    \item $O_Q[\mathbf{c}; Q_k]$ penalizes large amplitudes in the final solution,
    \begin{equation}
        O_Q[\mathbf{c}; Q_k] = \sum_{k=1}^K Q_k^2 A(\epsilon_k)^2 =
        \mathbf{c}^T \hat O_Q \mathbf{c}, \quad
        (\hat O_Q)_{jj'} = \sum_{k=1}^K Q_k^2 A_j(\epsilon_k) A_{j'}(\epsilon_k).
    \end{equation}

    \item $O_D[\mathbf{c}; D_k]$ penalizes large 1st derivatives of the final solution,
    \begin{equation}
        O_D[\mathbf{c}; D_k] = \sum_{k=1}^{K-1} D_k^2 |A'(\epsilon_k)|^2 =
        \mathbf{c}^T \hat O_D \mathbf{c}, \quad
        (\hat O_D)_{jj'} = \sum_{k=1}^{K-1} D_k^2 A'_j(\epsilon_k) A'_{j'}(\epsilon_k).
    \end{equation}

    \item $O_B[\mathbf{c}; B_k]$ penalizes large 2nd derivatives of the final solution,
    \begin{equation}
        O_B[\mathbf{c}; B_k] = \sum_{k=2}^{K-1} B_{k-1}^2 |A''(\epsilon_k)|^2 =
        \mathbf{c}^T \hat O_B \mathbf{c}, \quad
        (\hat O_B)_{jj'} = \sum_{k=2}^{K-1} B_{k-1}^2 A''_j(\epsilon_k) A''_{j'}(\epsilon_k).
    \end{equation}

    \item $O_T[\mathbf{c}; T_k, A_T(\epsilon)]$ penalizes deviations from a predefined default model (``target'') $A_T(\epsilon)$,
    \begin{align}
        O_T[\mathbf{c}; T_k, A_T(\epsilon)] &= \sum_{k=1}^K T_k
            [A(\epsilon_k) - A_T(\epsilon_k)]^2 =
            \mathbf{c}^T \hat O_T \mathbf{c} - 2\mathbf{f}_T^T \mathbf{c} + \mathrm{const},\\
        (\hat O_T)_{jj'} &= \sum_{k=1}^K T_k A_j(\epsilon_k) A_{j'}(\epsilon_k),
        \quad
        (\mathbf{f}_T)_j = \sum_{k=1}^K T_k A_j(\epsilon_k) A_T(\epsilon_k).
    \end{align}

    \item $O_U[\mathbf{c}; U]$ penalizes deviations from the equal weight superposition $c_j = 1/J$,
    \begin{align}
        O_U[\mathbf{c}; U] &= U \sum_{j=1}^J (c_j - 1/J)^2 =
        \mathbf{c}^T \hat O_U \mathbf{c} - 2\mathbf{f}_U^T \mathbf{c} + \mathrm{const},\\
        (\hat O_U)_{jj'} &= U \delta_{jj'},\quad
        (\mathbf{f}_U)_j = U / J.
\end{align}
\end{itemize}

Values of spectral functions $A_j(\epsilon)$ and their derivatives are computed on a user-supplied uniform or non-uniform energy grid $\{\epsilon_k\}$. The following finite-difference approximations are used for the derivatives,
\begin{align}
    A'_j(\epsilon_k) &\approx
        \frac{A_j(\epsilon_{k+1}) - A_j(\epsilon_k)}{\epsilon_{k+1} - \epsilon_k},\\
    A''_j(\epsilon_k) &\approx
        \frac{A_j(\epsilon_{k+1})(\epsilon_k - \epsilon_{k-1}) +
              A_j(\epsilon_{k-1})(\epsilon_{k+1} - \epsilon_{k}) -
              A_j(\epsilon_{k})(\epsilon_{k+1} - \epsilon_{k-1})}
             {(1/2)(\epsilon_{k+1} - \epsilon_{k-1})
                   (\epsilon_{k+1} - \epsilon_{k})
                   (\epsilon_{k} - \epsilon_{k-1})}.
\end{align}

The iterative optimization procedure proceeds as follows.
\begin{enumerate}
    \item Precompute values of $A_j(\epsilon_k)$, $A'_j(\epsilon_k)$ and $A''_j(\epsilon_k)$.

    \item Using user-supplied parameters $A_T(\epsilon_k) =$ \verb|default_model|, $T_k =$ \verb|default_model_weights| and\\ $U=$ \verb|ew_penalty_coeff|, precompute $\mathbf{f} = \mathbf{f}_T + \mathbf{f}_U$.

    \item Initialize regularization parameters according to
    \begin{align*}
        \mathcal{D} &= (\mathtt{der\_penalty\_init}) / J,\\
        Q_k &= 0 \\
        D_k &= \mathcal{D},\\
        B_k &= \mathcal{D}.
    \end{align*}

    \item Use the regularization parameters to compute matrix $\hat O = \hat O_Q + \hat O_D + \hat O_B + \hat O_T + \hat O_U$.
    
    \item Minimize a quadratic function
    \begin{equation}
        \mathbf{c}^T \hat O \mathbf{c} -2\mathbf{f}^T\mathbf{c}
    \end{equation}
    w.r.t. $\mathbf{c}$ subject to the normalization sum rule $\sum_{J=1}^J c_j = 1$. The minimization result at the $I$-th iteration is stored as a vector
    $\mathbf{c}^{(I)}$.

    \item Compute and store the relative $L_1$-distance between
    $\mathbf{c}^{(I)}$ and $\mathbf{c}^{(I-1)}$,
    \begin{equation}
        d(I, I-1) = \frac{\sum_{j=1}^J |c^{(I)}_j - c^{(I-1)}_j|}
                         {\sum_{j=1}^J |c^{(I)}_j|}.
    \end{equation}
    The initial vector $\mathbf{c}^{(0)}$ is taken to have all components equal to $1/J$.

    \item If $\sum_{j=1}^J |c^{(I)}_j| >$ \verb|max_sum_abs_c| (contribution of the negative coefficients is too big), then terminate iterations.

    \item If $d(I, I-1) < \min_n |\sigma_n/g_n|$, then convergence is reached and iterations are terminated. $g_n$ is the discretized right-hand side of the analytic continuation problem, and $\sigma_n$ are respective estimated error bars on the input data.

    \item Form a new final solution from coefficients $\mathbf{c}^{(I)}$,
    \begin{equation}
        A(\epsilon_k) = \sum_{j=1}^J c^{(I)}_j A_j(\epsilon_k),\quad
        A'(\epsilon_k) = \sum_{j=1}^J c^{(I)}_j A'_j(\epsilon_k),\quad
        A''(\epsilon_k) = \sum_{j=1}^J c^{(I)}_j A''_j(\epsilon_k).
    \end{equation}

    \item Increase regularization parameters $\mathcal{D}$, $D_k$ and $B_k$ by
    the factor \verb|der_penalty_coeff|.

    \item Update the amplitude regularization parameters $Q_k$: If $A(\epsilon_k) < 0$, then set $Q_k =$ \verb|amp_penalty_max|. Otherwise divide $Q_k$ by \verb|amp_penalty_divisor|.

    \item Update the 1st derivative regularization parameters $D_k$:
    If $D_k |A'(\epsilon_k)| > \mathcal{D}$, then set $D_k = \mathcal{D} / |A'(\epsilon_k)|$.

    \item Update the 2nd derivative regularization parameters $B_k$:
    If $B_k |A''(\epsilon_k)| > \mathcal{D}$, then set $B_k = \mathcal{D} / |A''(\epsilon_k)|$.

    \item If the maximal allowed number of iterations (\verb|max_iter|) is reached, terminate. Otherwise repeat from step 4.
\end{enumerate}

Once the iterations have been terminated, one inspects an accumulated list of pairs $(\mathbf{c}^{(1)}, d(1,0)), (\mathbf{c}^{(2)}, d(2,1)), \ldots$. Elements of the list are sorted in the ascending order according to $d(I,I-1)$, i.e. $\mathbf{c}^{(I)}$ from the iteration closest to the convergence comes first in the sorted list. The first vector of coefficients from the sorted list that obeys
\begin{equation}
    \chi^2\left[\sum_{j=1}^J c^{(I)}_j A_j\right] \leq \chi^2_c
\end{equation}
represents the sought final solution.

\section{Equality-constrained quadratic programming}
\label{ecqp}

Both consistent-constraints updates (Section \ref{cc_update}) and  construction of the final solution using the consistent-constraints protocol (Section II.B of \cite{socc_paper}) involve solving equality constrained quadratic programs. By definition, an equality-constrained quadratic programming is the process of minimizing a multivariate quadratic function
\begin{eqnarray}\label{ecqp:objective_function}
    \mathbf{x}^T \hat O \mathbf{x} - 2\mathbf{f}^T\mathbf{x}
\end{eqnarray}
w.r.t. a real vector $\mathbf{x}\in\mathbb{R}^N$ subject to a system of $P$ equality constraints
\begin{equation}\label{ecqp:constraints}
    \hat L \mathbf{x} = \mathbf{d}.
\end{equation}
Here, $\hat O$ is a real symmetric positive-definite $N\times N$ matrix, $\mathbf{f}\in\mathbb{R}^N$, $\hat L$ is a real $P \times N$ matrix, and $d \in \mathbb{R}^N$.

Such programs can always be reduced to equality-constrained least squares problems via the Cholesky decomposition of $\hat O$,
\begin{equation}
    \hat O = \hat A^T \hat A
\end{equation}
with an upper triangular $N\times N$ matrix $\hat A$, whose diagonal entries are positive. Let us consider an objective function of a general least-squares problem with the matrix $\hat A$ and a right-hand-side $\mathbf{b}\in \mathbb{R}^N$,
\begin{equation}
    ||\hat A\mathbf{x} - \mathbf{b}||_2 =
    (\hat A\mathbf{x} - \mathbf{b})^T (\hat A\mathbf{x} - \mathbf{b}) =
    \mathbf{x}^T \hat A^T \hat A \mathbf{x} -
    \mathbf{b}^T \hat A\mathbf{x} - \mathbf{x}^T \hat A^T\mathbf{b}
    + |\mathbf{b}|^2.
\end{equation}
By comparing this expression with (\ref{ecqp:objective_function}), we easily establish equivalence of the two objective functions,
\begin{equation}
    ||\hat A\mathbf{x} - \mathbf{b}||_2 =
    \mathbf{x}^T \hat O \mathbf{x} - 2\mathbf{f}^T\mathbf{x} + |\mathbf{b}|^2,
    \textrm{ where } \mathbf{f} = \hat A^T\mathbf{b}.
\end{equation}

A special class called \verb|ecqp_worker| solves the quadratic program
(\ref{ecqp:objective_function}--\ref{ecqp:constraints}) by calling a few LAPACK routines:
\begin{enumerate}
    \item Cholesky decomposition of matrix $\hat O$: LAPACK routine \verb|dpotrf|;

    \item Solution of the linear system $\hat A^T\mathbf{b} = \mathbf{f}$ to get the RHS $\mathbf{b}$: LAPACK routine \verb|dtrsm|.

    \item Equality-constrained minimization of the least squares objective function $||\hat A\mathbf{x} - \mathbf{b}||_2$: LAPACK routine \verb|dgglse|.
\end{enumerate}

\begin{thebibliography}{9}

\bibitem{som_lecture}
Eva Pavarini, Erik Koch, Frithjof Anders, and Mark Jarrell (eds.)
{\it Correlated Electrons: From Models to Materials
Modeling and Simulation}, Vol. 2, Chapter 14.
Verlag des Forschungszentrum Jülich, 2012,
ISBN 978-3-89336-796-2
\url{https://www.cond-mat.de/events/correl12/manuscripts/mishchenko.pdf}

\bibitem{socc_paper}
Olga Goulko, Andrey S. Mishchenko, Lode Pollet, Nikolay Prokof'ev, and Boris Svistunov.
{\it Numerical analytic continuation: Answers to well-posed questions},
\href{https://doi.org/10.1103/PhysRevB.95.014102}{Phys. Rev. B {\bf 95}, 014102 (2017)}.

\end{thebibliography}

\end{document}
